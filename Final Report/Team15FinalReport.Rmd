---
editor_options:
  markdown:
    wrap: 72
output:
  html_document:
    df_print: paged
  pdf_document: default
geometry: margin=.5in
fontsize: 11pt
always_allow_html: yes
---

```{r setup}

knitr::opts_knit$set(root.dir = here::here())
```

```{r}

source("Final Code/model_predictions.R")
source("Final Code/draw_confusion_matrix.R")
```

## MGT Madness - Team 15 Progress Report

Group Members: Michael Munson, Matthew Rosenthal, Matthew Royalty,
Jeffrey Sumner

## Overview of Project:

-   Team members names listed in heading
    -   Complete
-   Necessary background information/framing of the problem
    -   Complete
-   Include an overview of the problem in general as well as your
    general approach
    -   Could edit to only reflect predicting outcomes of games
-   State initial hypotheses
    -   Could edit to only reflect predicting outcomes of games

### Project Background:

\<Possibly reword to include all season!! Inspired by March Madness\>
[Royalty-added some comments about including regular season as well.]

Sports, especially the NCAA March Madness tournament, are popular for
entertainment and analysis. The tournament attracts millions of viewers
and spectators, generating significant revenue for the NCAA. In 2022,
the tournament had 10.7 million TV viewers, 685,000 attendees, and
generated an estimated \$1.15 billion in revenue [1].

March Madness and college basketball in general is a gold mine for
sports betting. Firms such as DraftKings and FanDuel earn a commission
with each bet made, in addition to revenue from their websites and app
ads. Reliable game prediction models are essential to maximizing this
revenue through improving lines and parlays, increasing the monetary
value of bets made, and encouraging more bettors to use their website
and app.

Predicting the Men's Division I March Madness Tournament outcome is a
challenging task for analytics enthusiasts. It involves forecasting 67
games played over three weeks, including possible upsets, player
injuries, and changing data. Achieving certainty in this prediction
process is difficult. Given the interest in college basketball
throughout the season, our team decided to expand our project to include
regular season NCAA games as well.

To predict the outcomes of matchups in March Madness, various analyses
are conducted. Live win probabilities, calculated on a play-by-play
basis, consider factors like remaining game time, score difference,
possession, and pre-game win probabilities. The excitement index
measures the rate of change of a team's chance of winning during a game,
impacting viewership. Elo ratings, which rate a team's chances of
winning based on factors like location, conference, and game type, are
also used.

The goal of MGT Madness is to create machine learning models that
reliably predict the outcome of NCAA games for both March Madness and
regular season given different match ups between teams, other pre-game
characteristics and team statistics. The main driving question was: What
predictors can be used to accurately predict the outcome of NCAA
tournament games?

To answer this question, we planned to: collect large amounts of
disparate data on games in the past, process the data while also
creating unique features, train and validate multiple types models, and
finally compare these models against each other on various methods. The
models will then be combined into one ensemble model...

#### Modeling Game Results:

Since basketball game results are binary (either a win or loss), a
probit or logistic regression model is great fit. We will compare
potential models using accuracy, precision, and specificity, among other
statistics.

Potential factors to investigate include:

-   Team ranking differential
-   Team's quality regular season wins
-   Game location relative to each team's campus
-   Game's round in tournament
-   Team efficiency (eg. offensive points per 100 possessions and points
    allowed per 100 possessions)
-   Statistics over the last 5 games

We anticipate that each team's ranking differential, quality wins,
location, and efficiency will be the biggest factors determining the
win/loss outcome of a game.

## Overview of Data:

-   How involved was the cleaning process?
    -   Could expand more on this, but have a good start.
-   What were your key variables?
    -   If this is asking for stuff from results, then we would still be
        wanting to include this
    -   If this is about our initial hypothesis, then this is complete
-   Any interesting insights from EDA?
    -   add some pca and UMAP with tidymodels...
-   If you used feature engineering how and was it successful?
    -   Discussion of distance, roll 5, season ave...
    -   Roll5... the goal of using the last 5 games was to capture
        team's current gameplay. This was our attempt at capturing team
        momentum (such as win or losing streaks).
    -   Offensive and Defensive Efficiency: We created predictors for
        team offensive and defensive efficiency. The efficiency
        predictors provide useful measures of a team's overall
        performance by considering both offensive and defensive
        statistics while not skewing the offensive and defensive metrics
        based on a team's pace of play. Offensive efficiency is
        calculated using offensive points per 100 possessions. Defensive
        efficiency is calculated using defensive points allowed per 100
        possessions. Possession data wasn't inherently available in our
        datasets. We engineered possessions using the following formula.
        The 0.44 factor was suggested in by kenpom. **\<need
        reference\>**
        -   Team Possessions = (team field goal attempts - team
            offensive rebounds) + team total turnovers + (0.44 \* team
            free throw attempts)
-   Where did the dataset come from?
    -   Complete
-   Super quick overview of the data
    -   Complete

### Data Preparation and Cleaning

The project involved gathering data on game outcomes, betting odds,
attendance, and AP poll rankings for college basketball games between
2012 and 2022. The data was collected using a combination of web
scraping and querying APIs.

The data collection process began with using datasets from the
`ncaahoopR` library, which included box score, play-by-play, rosters,
and schedule files. The resulting data frame served as the basis for
amending and creating other features.

Similarly, we used the `hoopR` package to retrieve betting data for each
game using the `espn_mbb_betting(<game_id>)` function. Due to the size
of the data files, this data had to be stored within three different
files.

To collect data on AP poll rankings, the we wrote a web-scraping
function that collected data from sports-reference.com using the `rvest`
package. The data was then processed using the `tidyverse` package and
`lapply()` function to combine scraped data frames into a single table.

For attendance data, we used the `game_id` identifier to query the ESPN
API and retrieve data on attendance. We also attempted to use the
`hoopR` package, which had data in its repository files, but ultimately
used the ESPN API.

Finally, we processed and merged all of the collected data into one data
frame using the `data_cleanup.R` script. The script included processing
steps such as filtering and renaming variables, geocoding college
locations, and merging data frames.

#### Data Source Overview:

-   ESPN API has a wealth of information ranging from team information
    (location, logo, colors, jerseys, etc.) to box-score and game-time
    information
-   sports-reference.com contains AP poll ranking data in addition to
    other ranking sources
-   The ncaahoopR data contains information around box scores,
    play-by-play and team information. The box scores and play-by-play
    data will be instrumental for our project. This will be one of the
    main sources for key statistics such as points scored, rebounds,
    steals, win probabilities, etc. Below is an example of one of the
    box scores
    -   <https://github.com/lbenz730/ncaahoopR>
    -   <https://github.com/lbenz730/ncaahoopR_data>
-   NCAA Men's Basketball Data: - Records from 2000, including game
    attendance, team records per season, week-by-week Associated Press
    Poll Records - These records are mostly in .pdf format
    -   <https://www.ncaa.org/sports/2013/11/27/ncaa-men-s-basketball-records-books.aspx>
-   The Kaggle datasets include: - Basic information: Team ID's and
    Names; Tournament seeds since 1984; final scores of all regular
    season, tournament games; etc. - Team Box Scores: game-by-game stats
    at a team level (free throws, rebounds, turnovers, etc.) -
    Geography: the city locations of all games since 2009 - Public
    Rankings: Weekly team rankings from multiple metrics - Supplemental
    Information: Coaches, conference affiliations, bracket structure,
    etc.
    -   <https://www.kaggle.com/competitions/mens-march-mania-2022/data>

    -   <https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
if(!require(kableExtra)) install.packages('kableExtra')
if(!require(devtools)) install.packages('devtools')
if(!require(ncaahoopR)) devtools::install_github('lbenz730/ncaahoopR')
if(!require(tidyverse)) install.packages('tidyverse')
sample_data <- ncaahoopR::get_boxscore(401168364)

duke_sample <- sample_data$Duke %>%
  select(-player,-home,-opponent,-team,-starter)

duke_sample %>%
  head(3) %>%
  kable() %>%
  kable_styling("striped")
```

-   Because we were scraping data from ESPN and other sources, there
    were times when we were throttled and even blocked from pulling
    data. This was an unforeseen issue that ultimately required our team
    to take a step back and rethink our approach. We found a publicly
    available ESPN API that allowed us to pull the necessary data in a
    more efficient manner than web scraping.
    -   There are many different ways for us to clean our data and one
        of the biggest challenges is determining what is right for our
        project. Do we want to use stats based on the last 5 games, 10
        games? Do we want to focus solely on how a single team performs
        or do we want to compare how the team performs vs how the
        opponent performs? These are the kinds of questions that we've
        asked ourselves to determine how the data will be formatted for
        analysis.
-   For an in-depth look at how we combined our code and sources, please
    refer to our Team 15 github.

**Data Exploration:**

We created a correlation matrix of our factors. Due to how we calculated
some of our features, many of the relationships were not surprising such
as the highly correlated rating factors. However, things like points
scored over the last 5 games didn't correlate with total turnovers in
the last 5 games as strongly as expected. Meaning that points off
turnovers isn't as large of a factor to points scored as anticipated.

![Data Exploration - Correlation Plot of Win Probability
Predictors](../Progress%20Report/images/Data%20Exploration%20-%20Correlation%20Plot%20of%20Win%20Probability%20Predictors-01.png)

In addition to our correlations, we examined the distributions of
factors to determine how well we hit normality assumptions and for
scaling purposes in later analyses. Due to the large size of the data,
our predictors were approximately normal.

![Data Exploration - Density Plot of Win Probability
Predictors](../Progress%20Report/images/Data%20Exploration%20-%20Density%20Plot%20of%20Win%20Probability%20Predictors.png)

## Overview of Modeling

Now that we have explored our data, we are ready to begin modeling it.
As stated previously, our goal is to model the outcome, of any
particular NCAA basketball game. Our response variable is whether or not
the home team wins or loses the game. We will explore common techniques
used to predict win probabilities such as logistic regression, probit
regression and decision tree classification. In order to do this, we've
broken our modeling out into multiple parts:

1.  Splitting the data
2.  Baseline models
3.  Feature selection
4.  "Simple" models with feature selection
5.  "Enhanced" models with feature selection

-   ~~What type of models did you use and how do they compare?~~
-   ~~\<Could add more here to give an overview... check to see if
    redundant with above introduction sections\>~~
-   ~~Multiple parts~~
    -   ~~Splitting data~~
    -   ~~Non-LASSO models ("Baseline models")~~
        -   ~~Train season tbl -\> All 95 features~~
    -   ~~LASSO~~
    -   ~~Creating LASSO "simple" models~~
    -   ~~Using LASSO selected features + normalization + PCA (\>80%
        explained variance) to create new models ("enhanced models")~~

#### Splitting Data

To ensure the validity of our models and to help avoid the risk of
overfitting, we randomly split our data into train, test, and validation
sets with 60% going into the training set, 20% into the test set and 20%
into the validation set. The validation set provides us a way to rank
our model's accuracy and aid in model selection. The test set provides
an evaluation of our final model. Below is a summary of each dataset
with the number of observations, total wins and winning percentage to
understand the distribution of wins across each.

```{r}
bind_rows(
  train_tbl %>% select(home_winner_response) %>% mutate(type = "Train")
  , validation_tbl %>% select(home_winner_response) %>% mutate(type = "Validation")
  , test_tbl %>% select(home_winner_response) %>% mutate(type = "Test")
) %>%
  group_by(type) %>%
  summarize(Wins = sum(home_winner_response %>% as.numeric() -1)
            , `Total Games` = n()
            , `Winning Percentage` = Wins/`Total Games`
            )
```

The split above does appear to be even as far as winning percentage goes
and therefore no varying bias across the different types that could
throw off our modeling. However, we do see that we have a clear bias
towards wins as essentially each dataset has a 63% `Winning Percentage`.
This may lead us to over-predict wins across all types.

### Baseline Modeling

With our data split into train, test and validation, we want to predict
outcomes using all features. This exercise is meant to give us something
to compare to and hopefully beat as we attempt to improve out models.
For the purposes of this paper, we will focus on the comparing the test
unless specified otherwise.

#### ESPN Baseline

Let's first examine the ESPN baseline. ESPN predicts outcomes before and
during most sporting events. Below is the contingency table and
associated metrics:

```{r}
espn_prob_tmp <- test_tbl %>% 
  select(home_espn_probability, home_winner_response) %>%
  mutate(home_espn_probability = round(home_espn_probability/100)
         , home_winner_response = home_winner_response %>% as.numeric() -1 
         )

caret::confusionMatrix(reference = espn_prob_tmp$home_winner_response  %>% factor(), data=espn_prob_tmp$home_espn_probability  %>% factor()) %>%
  draw_confusion_matrix()

```

Based on these results, the ESPN baseline accuracy is roughly 72%.

#### Team 15 Baseline

To establish a performance baseline for our own win probability
predictive models, we created a simple logistic regression model using
all parameters in our dataset. In all likelihood, we anticipate a decent
model albeit very overfit. Below are the results:

```{r}
model_tbl %>% filter(Simplicity == "baseline", Metric == "season", `Model Type` == "logit") %>% pull(test_confusion_matrix) %>% .[[1]] %>% draw_confusion_matrix()
```

Based on the results, our baseline accuracy is roughly 70%. Overall,
this is a decent model but is likely to be overfit and unstable
long-term.

\-\-\-\-\-\-\-\-\-\-\-\--

\<Amend to reflect new Final Code, different games...\> For our simple
exploratory model, we used a logit model with only a few of our features
. Our initial win probability model was a logistic regression model
through the `glm(, family='binomial')` function in R. For this model, we
only considered a small amount a features. The confusion matrix below
shows that our Logit model's accuracy = 65.35%, precision = 65.78%,
specificity = 63.59%. This model helped us get our bearings and set a
baseline for our own improvements.

|             | Predicted Win | Predicted Loss |
|-------------|---------------|----------------|
| Actual Win  | 33,193        | 16,317         |
| Actual Loss | 17,267        | 30,154         |

: Logit Model Confusion Matrix (omitting the first 5 games of each team
of each season)

<Amend to reflect new Final code> We also created a simple probit model.
Like the previous logit model, our goal was to create a baseline in
performance. Unlike the previous logit model, we used a large amount of
features (99 features). Two of these models were created; one where the
game statistics were from the season average, the other with game
statistics from only the last 5 games. For these probit models, the
season averages provided a slightly better prediction of game outcomes
than the last 5 games averages. [could be a table... Roll5: acc = 0.704,
prec = 0.628, spec = 0.832, auc = 0.658, AIC = 30616... Season Average:
acc = 0.717, prec = 0.644, spec = 0.832 (same), auc = 0.676, AIC =
29824]. Although the probit model trained on the season averages data
was better, it didn't outperform the last 5 games averages by enough for
us to abandon the last 5 games averages as a separate data set.

<Probably stays same> As expected, ESPN does outperform our initial
logit and probit models. This will provide us with a good baseline to
reference for our future modeling attempts.

\<Add statement, does season avg outperform roll5?\>

\<Still a valid statement, might omit this later...\> Of note, the
coefficient of the factor of home_distance / away_distance from our
logit model initially confirmed the common assumption that there is a
home field advantage. These results may change as we continue to develop
our models.

#### Feature Selection Exploration

-   <To be updated / deleted> To find the best lambda value for LASSO,
    we used `cv.glmnet(family = "binomial", alpha = 1, nfolds = 10)`. We
    then used the coefficients using the lambda value that minimized the
    cross-validation error to select our significant predictors from an
    original list of 49 potential predictors. These significant
    predictors were then used to create logit models. Altering the
    syntax for the coefficient cutoff changed the selected predictors.
    For instance, using the input `coefficient!=0` resulted in 38
    predictors (accuracy = 0.6751, precision = 0.7010, specification =
    0.5941, auc = 0.7104). Using `coefficient>0` surprisingly resulted
    in 17 predictors and had the same performance as the previous
    syntax. Ultimately, we could set `coefficient>10^-3` before seeing a
    drop in performance (resulted in 15 predictors, accuracy = 0.6750,
    precision = 0.7005, specification = 0.5944, auc = 0.7098).According
    to the LASSO method, ... were the most important features

    -   Also, coefficient cutoffs were limited to coefficients with
        magnitudes larger than 0.05
    -   season results: 95 to 46 features...
        -   ratings made the cut, but home-distance traveled did NOT
            make the cut!
    -   roll5: 95 to 19 features...
        -   Still no distances selected.

            The training set was used to fit our lasso model which was
            created to select our optimal set of predictors. Using the
            `cv.glmnet` function, we performed 10-fold cross-validation
            on our lasso model. Cross-validation is a technique used to
            evaluate the performance of a model by splitting the dataset
            into multiple parts, training the model on one part, and
            testing it on another. This process is repeated multiple
            times, with different parts of the dataset used for training
            and testing each time.\
            \
            After predictors were selected with the lasso model model,
            we then trained each of our models with the training
            dataset. The performance was then evaluated with the
            validation set to insure it did not overfit the training
            data. Finally, performance of the model was evaluated using
            the test set. The test set was held out during the modeling
            process to ensure an unbiased representation of each model's
            perforance.

-   [show vip of logit model] [Jeffrey]
```{r}
vip(baseline_rolling_logit_model)
```

    \<probably omit this paragraph, but mention\>

-   In order to explore feature selection, we used stepwise regression
    through the `MASS` package. The steps were bidirectional elimination
    with AIC as the model improvement criterion.<awk wording> This
    method resulted in 23 features (from 49) being selected. The logit
    model trained on these features had a performance that includes:
    accuracy = 0.7000, precision = 0.7238, specification = 0.6326, and
    AUC = 0.7457. According to stepwise method... were the most
    important features.
    <potentially add how this compares / contrasts to the previous probit or logit models above>
    [Matthew Rosenthal, Michael] \<if you are interested in seeing the
    stepwise... go to ... file\>

-   As we were exploring methods for predictor selection, we looked at
    stepwise regression as well as LASSO regression. We eventually
    settled on LASSO regression due to overall performance of stepwise
    regression. Running stepwise regression using large number of
    predictors was not as efficient as LASSO regression due to how
    stepwise regression interactively adds and removes predictors based
    on metrics until a subset of predictors is found. LASSO regression
    provided us with a subset of predictors without sacrificing
    performance training our models.

## Simple Models

-   

    ```         
    <Explain Simple Models how built>
    ```

-   Logit results

    -   rolling, season
    -   Confusion Table

-   Probit results

    -   season, mention rolling

-   xgb trees results

    -   season, mention rolling

## Enhanced Models

\<Paragraph here possibly... if not covered above: what is meant by
"enhanced?"\> - Logit results - Confusion Table - Probit results - Xgb
Trees results

## Ensemble

-   Finally, we created an ensemble model which combined all of our
    models [see on Wednesday to check]

    -   Confusion Table

-   How did you perform model selection and hyperparameter optimization?

    -   In the "conclusions" for model selection
    -   In the tree and svm for hyperparameters

-   How did the models perform generally speaking?

    -   reflected in the accuracy, precision, ...

-   Are they useful and in what ways?

    -   We might have to revisit the sections to include this! [Goes
        into a conclusion section]
        -   our model(s) could be combined with ESPN's predictions...

-   Detailed discussion and evaluation of results.

    -   Most likely will be found in the conclusions section
    -   Also included with the model results.

-   Overall conclusion and key takeaways from your project as a closing
    message.

    -   needed in the conclusion

-   If you encounter any unexpected problems, challenges, or interesting
    findings please mention these.

    -   "Standard Dataset" How it was built...
    -   How it performed (overfit)
    -   Why we dropped it.

-   Is there any unfinished business or areas which if given more time
    or resources you would deem promising or interesting to further
    pursue?

    -   Attendance predictions!
    -   Time between games
    -   Direction of Travel
    -   Geographical Bias of Ratings
    -   Individual Player Stats (star player effect)
    -   Injuries Data
    -   Continue analysis with stepwise feature selection instead of
        LASSO, see if results differ.

### Works Cited

-   [1] Bubel, Jennifer. "How Much Money Do Universities Get for Going
    to the NCAA March Madness Tournament?" *Diario AS*, 28 Feb. 2023,
    <https://en.as.com/ncaa/how-much-money-do-universities-get-for-going-to-the-ncaa-march-madness-tournament-n/.>

-   [2] Parker, Tim. "How Much Does the NCAA Make off March Madness?"
    Edited by Jefreda R Brown, *Investopedia*, Investopedia, 9 Mar.
    2023,
    <https://www.investopedia.com/articles/investing/031516/how-much-does-ncaa-make-march-madness.asp#:~:text=In%202022%2C%2045%20million%20Americans,see%20the%20heftiest%20cash%2Dout.>

-   [3] Pomeroy, Ken. "The Possession", *Kenpom*, 19 Mar. 2004,
    [https://kenpom.com/blog/the-possession/#:\\\~:text=The%20most%20common%20formula%20for,and%20FTA%20%3D%20free%20throw%20attempts](https://kenpom.com/blog/the-possession/#:~:text=The%20most%20common%20formula%20for,and%20FTA%20%3D%20free%20throw%20attempts){.uri}

-   [4] Korpar, Lora. "March Madness Betting Expected to Exceed \$3
    Billion, Set All-Time High", *Newsweek*, 14 Mar. 2022,
    <https://www.newsweek.com/march-madness-betting-expected-exceed-3-billion-set-all-time-high-1687917>

-   [5] Coleman, Madeline. "March Madness: How a fan used a machine to
    nail his bracket", *Sports Illustrated*, 31 Mar. 2021,
    <https://www.si.com/college/2021/03/31/march-madness-fan-trained-machine-predict-bracket-will-geoghegan>

-   [6] Consoli, John. "Advertisers Go Mad For March Madness", *TV News
    Check,* 16 Mar 2022,
    <https://tvnewscheck.com/business/article/advertisers-go-mad-for-march-madness/>

## Project Final Report, Data, & Code

#### What is it?

It will be a detailed description of what you did, what results you
obtained, clear interpretations of the results and what you have learned
and/or can conclude from your work.

All deliverables and work created throughout your project including
code, notebooks, reports, etc.

Imagine this deliverable as the official and final project in its
entirety. This would be what you deliver to your client/manager on the
completion of a project.

#### What to Include & What is Required?

"Academic-like" paper with dense text inline figures, no direct code
within the paper. Code files related to the project are included in the
repository files but are self-contained. Detailed instructions are
provided accordingly. Any relevant code files Key visualizations or
other supplementary files

Readme/Documentation for code

## Overall Considerations:

-   Include a couple of key visualizations inline and be sure to include
    captions, labels, legends, and most importantly context where
    needed!

    -   Planned

    -   Possibly mention problems with the data with "\_" at the end.

    -   Possibly mention runtime problems for `MASS` stepwise

-   Discussion of things that didn't work is also encouraged.

-   Detailed discussion of methodology.

    -   Possibly add coding description for data cleaning
        -   In earlier HEAD of Progress Report
    -   Should have methodology of modeling

-   Describe in depth the novelties of your approach and your
    discoveries/insights/experiments.

    -   Better callouts could be added for feature engineering
        -   Roll5, distance

## Code Files

-   Code used to generate the project and accompanying figures.
-   Include a Readme with installation and running instructions as well
    as an overview of your directory structure and any other important
    elements.
-   Requirements.txt/.yml file if you are using libraries outside of
    those used in the course.
-   Any extra documentation/explanation for a user/TA to run the code to
    see the results or ideally a notebook that shows all the results in
    a logical sequence

## Data

-   See the "Submitting Data" section for details on how to submit your
    dataset

## Submission

-   Group Primary Contact will submit your Project Final Report, Data,
    Slides, & Code on behalf of the group via the corresponding
    assignment on Canvas/Edx. The easiest way to do this is to download
    your entire GitHub repository and upload that resulting zip file as
    your submission. This cloned repository should contain all the
    necessary deliverables mentioned above.

## Grading

-   TA feedback/grades on Project Final Report, Data, Slides, & Code
    will be available prior to the conclusion of the course.

### Next Steps <From Progress Report>

The next steps involve building different models and performing
validation for a dataset. The models that will be built include
correlation analysis, principal component analysis, linear regression,
and decision trees. These models will be used to predict attendance and
win/loss outcomes in the dataset.

For **attendance prediction**, the Mean Squared Error (MSE) metric will
be used for model validation. MSE measures the average squared
difference between the predicted attendance values and the actual
attendance values in the dataset.

For **win/loss prediction**, the dataset will be split into training,
testing, and validation sets. The training set will be used to train the
model, the testing set will be used to evaluate the model's performance
on unseen data, and the validation set will be used to evaluate the
model's final performance.

In addition to the train/test/validate splits, cross-validation will
also be performed. Cross-validation is a technique used to evaluate the
performance of a model by splitting the dataset into multiple parts,
training the model on one part, and testing it on another. This process
is repeated multiple times, with different parts of the dataset used for
training and testing each time.

Finally, accuracy and recall metrics will be used to evaluate the
performance of models predicting win/loss outcomes. Accuracy measures
the proportion of correctly predicted outcomes, while recall measures
the proportion of actual outcomes that were correctly predicted.

We expect our modeling next steps to be completed by April 8th in order
to prepare our final presentation and summary of results.
