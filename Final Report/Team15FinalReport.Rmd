---
editor_options:
  markdown:
    wrap: 72
output:
  html_document:
    df_print: paged
  pdf_document: default
geometry: margin=.5in
fontsize: 11pt
always_allow_html: yes
---

```{r setup, echo=FALSE}

knitr::opts_knit$set(root.dir = here::here())
```

```{r source_files, echo=FALSE, warning=FALSE, message=FALSE}
if(!require(kableExtra)) install.packages('kableExtra')
if(!require(devtools)) install.packages('devtools')
if(!require(ncaahoopR)) devtools::install_github('lbenz730/ncaahoopR')
if(!require(tidyverse)) install.packages('tidyverse')
source("Final Code/model_predictions.R")
source("Final Code/draw_confusion_matrix.R")
```

## MGT Madness - Team 15 Progress Report

Group Members: Michael Munson, Matthew Rosenthal, Matthew Royalty,
Jeffrey Sumner

## Overview of Project:

### Project Background:

Every March, millions of Americans come together to watch the NCAA
Basketball Championships, AKA "March Madness". The college tournament
attracts, which consists of 67 games spread over a 3-week period, has
millions of viewers and spectators, generating significant revenue for
the NCAA. In 2022, the tournament had 10.7 million TV viewers, 685,000
attendees, and generated an estimated \$1.15 billion in revenue [1].

Naturally, March Madness is a gold mine for sports betting. Firms such
as DraftKings and FanDuel earn a commission with each bet made, in
addition to revenue from their websites and app ads. In order to
encourage bettors to their sites and to maximize this revenue, betting
companies must have the most accurate models possible. This is a
challenging task for even the most seasoned data analysts; games are
influenced by a near-infinite number of factors, ranging from "obvious"
ones like team statistics to more abstract ones like injuries, momentum,
and crowd noise.

The goal of MGT Madness was to create a machine learning model that
reliably predicted the outcome of both March Madness and regular season
college basketball games at a level equal or better to ESPN's
industry-standard prediction models. To achieve this, our main driving
question was: What predictors could be used to accurately predict the
outcome of a college basketball game?

To answer this question, we collected large amounts past game data,
processed the data, selected influential features, trained and validated
numerous models, and finally compared these models against each other on
various methods.

## Overview of Data:

### Data Preparation and Cleaning

The project involved gathering data on game outcomes, betting odds, and AP poll rankings for college basketball games between 2012 and 2022. The data was collected using a combination of web scraping and querying APIs.

The data collection process began with using datasets from the
`ncaahoopR` library, which included box score, play-by-play, rosters,
and schedule files. The resulting data frame served as the basis for
amending and creating other features.

Similarly, we used the `hoopR` package to retrieve betting data for each
game using the `espn_mbb_betting(<game_id>)` function. Due to the size
of the data files, this data had to be stored within three different
files.

To collect data on AP poll rankings, the we wrote a web-scraping
function that collected data from sports-reference.com using the `rvest`
package. The data was then processed using the `tidyverse` package and
`lapply()` function to combine scraped data frames into a single table.

Finally, we processed and merged all of the collected data into one data
frame using the `data_cleanup.R` script. The script included processing
steps such as filtering and renaming variables, geocoding college
locations, and merging data frames.

Not all of the data collection went smoothly. There were times when ESPN and other sources throttled or even blocked  us from pulling data. This was an unforeseen issue that ultimately required our team
to take a step back and rethink our approach. We found a publicly
available ESPN API that allowed us to pull the necessary data in a more efficient manner than web scraping.

Once we had our data, we had to determine the right ways to clean our data given our project needs and objectives. We explored many questions, including: Do we want to use statistics based on the last 5 or 10
games? Do we want to focus solely on how a single team performs
or do we want to compare how the team performs vs how the
opponent performs?

For an in-depth look at how we combined our code and sources, please refer to our Team 15 github.

#### Data Source Overview:

-   ESPN API has a wealth of information ranging from team information
    (location, logo, colors, jerseys, etc.) to box-score and game-time
    information
-   sports-reference.com contains AP poll ranking data in addition to
    other ranking sources
-   The ncaahoopR data contains information around box scores,
    play-by-play and team information. The box scores and play-by-play
    data will be instrumental for our project. This will be one of the
    main sources for key statistics such as points scored, rebounds,
    steals, win probabilities, etc. Below is an example of one of the
    box scores
    -   <https://github.com/lbenz730/ncaahoopR>
    -   <https://github.com/lbenz730/ncaahoopR_data>
-   NCAA Men's Basketball Data: - Records from 2000, including game
    attendance, team records per season, week-by-week Associated Press
    Poll Records - These records are mostly in .pdf format
-   <https://www.ncaa.org/sports/2013/11/27/ncaa-men-s-basketball-records-books.aspx>
-   The Kaggle datasets include: - Basic information: Team ID's and
    Names; Tournament seeds since 1984; final scores of all regular
    season, tournament games; etc. - Team Box Scores: game-by-game stats
    at a team level (free throws, rebounds, turnovers, etc.) -
    Geography: the city locations of all games since 2009 - Public
    Rankings: Weekly team rankings from multiple metrics - Supplemental
    Information: Coaches, conference affiliations, bracket structure,
    etc. - <https://www.kaggle.com/competitions/mens-march-mania-2022/data>
    - <https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset>


```{r SampleData, echo=FALSE, warning=FALSE, message=FALSE}

sample_data <- ncaahoopR::get_boxscore(401168364)

duke_sample <- sample_data$Duke %>%
  select(-player,-home,-opponent,-team,-starter)

duke_sample %>%
  head(3) %>%
  kable() %>%
  kable_styling("striped")
```

#### Data Exploration:

We created a correlation matrix of our factors. Due to how we calculated
some of our features, many of the relationships were not surprising such
as the highly correlated rating factors. However, things like points
scored over the last 5 games didn't correlate with total turnovers in
the last 5 games as strongly as expected. Meaning that points off
turnovers isn't as large of a factor to points scored as anticipated.

```{r CorrelationEDA, echo=FALSE, warning=FALSE, message=FALSE}
train_tbl %>%
  ungroup() %>%
  select(contains("season"), -contains("FTM"), -contains("FGA"), -contains("FGM"), -contains("FTA"), -contains("team"),-contains("_opp"), -contains("fouls")) %>%
  # pivot_longer(everything()) %>%
  cor() %>%
  ggcorrplot::ggcorrplot(type = "lower", lab = FALSE)
```

In addition to correlations, we examined the distributions of
factors to ensure that they met normality assumptions and could be effectively scaled later analyses. 
Fortunately, many of our predictors were approximately normal. However there were some predictors that had centers very close to zero and therefore had non-normal distributions. This included game statistics
like flagrant and technical fouls.

```{r DensityEDA, echo=FALSE, warning=FALSE, message=FALSE}
train_tbl %>%
  ungroup() %>%
  select(contains("season"), -contains("FTM"), -contains("FGA"), -contains("FGM"), -contains("FTA"), -contains("team"),-contains("_opp")) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value)) +
  geom_density() +
  facet_wrap(.~name, scales = 'free') +
  labs(title = "Density Plot of Win Probability Predictors") +
  ggthemes::theme_fivethirtyeight() 
```

Initially, we explored modeling with features centered close to zero. However, in some of the modeling attempts, these features would be selected and negatively impact the model's performance. For instance, models trained with features with centers close to zero would sometimes predict only one class of outcome, such as all wins or all losses. To improve performance, we removed such features, resulting in a reduction from 286 to 237 remaining features.


## Feature Engineering

We created predictors for team offensive and defensive efficiency. The
efficiency predictors provide useful measures of a team's overall
performance by considering both offensive and defensive statistics while
not skewing the offensive and defensive metrics based on a team's pace
of play. Offensive efficiency is calculated using offensive points per
100 possessions. Defensive efficiency is calculated using defensive
points allowed per 100 possessions. Possession data wasn't inherently
available in our datasets. We engineered possessions using the following
formula. The 0.44 factor was suggested by kenpom.

To provide a more up-to-date representation of team performance we
created a rolling 5 game average for each game statistical variable such
as points per game, rebounds per game, offensive/defensive efficiency,
etc.) for a team over their last five games. By doing so, we aimed to
reduce noise in the data compared to a season average, identify trends
when a team is performing well, and quickly react to changes such as
player injuries or other changes to a team's roster.

In addition to using rolling 5 game averages, we also utilized season
average predictors to gain a broader understanding of team performance.
To create these predictors, we calculated the average of various team
game statistical variables for a team over the entire season. For each
of these, we summed the total number of points (or rebounds, assists,
etc.) accumulated and dividing it by the number of games played. We
hypothesized season average predictors to be useful in providing a more
holistic view of a team's performance.

By using both rolling 5 game averages and season average predictors, we
aimed to gain a more comprehensive understanding of team performance.

<needs fact check!!!>
The last feature we created was distance traveled to the game site. If the team was playing a home game, their distance traveled would be zero. To create this feature, we collected data on game site longitude and latitude. The distance calculated was the straight-line <(ellipsoid)> distance through the <`geosphere()`> function.


After our data collection, cleaning and feature engineering, we had a dataset with 45,518 data points; each representing a single game. Each data point contained 237 features which contained game statistics (rebounds, field goal attempts, etc.) and our engineered features.


## Overview of Modeling

As stated previously, the goal was to model the outcome, of any
particular NCAA basketball game. Our response variable was whether or
not the home team wins or loses the game. We explored common techniques
used to predict win probabilities such as logistic regression, probit
regression and decision tree classification. In order to do this, we
split our modeling out into multiple parts:

1.  Splitting the data

2.  Baseline models

3.  Feature selection

4.  "Simple" models with feature selection

5.  "Enhanced" models with feature selection

6.  All model comparison

For each of these sections we chose to focus on a single mode, the
season average logistic regression model. We will examine all models in
the All model comparison section.

#### Splitting Data

To ensure the validity of our models and to help avoid the risk of
overfitting, we randomly split our data into train, test, and validation
sets with 60% going into the training set, 20% into the test set and 20%
into the validation set. The validation set provides us a way to rank
our model's accuracy and aid in model selection. The test set provides
an evaluation of our final model. Below is a summary of each dataset
with the number of observations, total wins and winning percentage to
understand the distribution of wins across each.

```{r split_data, echo=FALSE, warning=FALSE, message=FALSE}
bind_rows(
  train_tbl %>% select(home_winner_response) %>% mutate(type = "Train")
  , validation_tbl %>% select(home_winner_response) %>% mutate(type = "Validation")
  , test_tbl %>% select(home_winner_response) %>% mutate(type = "Test")
) %>%
  group_by(type) %>%
  summarize(Wins = sum(home_winner_response %>% as.numeric() -1)
            , `Total Games` = n()
            , `Winning Percentage` = Wins/`Total Games`
  )
```

The split above does appear to be even as far as winning percentage goes
and therefore no varying bias across the different types that could
throw off our modeling. However, we do see that we have a clear bias
towards wins as essentially each dataset has a 63% `Winning Percentage`.
This may lead us to over-predict wins across all types.

-   Data was also segmented by the type of averaging (mentioned above).
    Either rolling or season...

### Baseline Modeling

With our data split into train, test and validation, we want to predict
outcomes using all features. This exercise is meant to give us something
to compare to and hopefully beat as we attempt to improve out models.
For the purposes of this paper, we will focus on the comparing the test
unless specified otherwise.

#### ESPN Baseline

Let's first examine the ESPN baseline. ESPN predicts outcomes before and
during most sporting events. Below is the contingency table and
associated metrics:

```{r espn_baseline, echo=FALSE, warning=FALSE, message=FALSE}
espn_prob_tmp <- test_tbl %>% 
  select(home_espn_probability, home_winner_response) %>%
  mutate(home_espn_probability = round(home_espn_probability/100)
         , home_winner_response = home_winner_response %>% as.numeric() -1 
         )

caret::confusionMatrix(reference = espn_prob_tmp$home_winner_response  %>% factor(), data=espn_prob_tmp$home_espn_probability  %>% factor()) %>%
  draw_confusion_matrix()

```

Based on these results, the ESPN baseline accuracy is roughly 72%.

#### Team 15 Baseline

To establish a performance baseline for our own win probability
predictive models, we created a simple logistic regression model using
all parameters in our dataset for both rolling averages and season
averages. We anticipated a decent model albeit very overfit. Below were
the results for the season averaged logit model. The results for the
rolling averages dataset were comparable and will be examined further in
our paper:

```{r team15_baseline, echo=FALSE, warning=FALSE, message=FALSE}
model_tbl %>% filter(Simplicity == "baseline", Metric == "season", `Model Type` == "logit") %>% pull(test_confusion_matrix) %>% .[[1]] %>% draw_confusion_matrix()
```

Based on the results, our baseline accuracy built upon season averages
is roughly 70%. Overall, this is a decent model but is likely to be
overfit and unstable long-term.

<this moved to the massive comparison portion> This shows that the
rolling averages perform roughly as well as the season averages. We
continued creating parallel models to further explore the effect of
rolling averages vs season averages.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# data.frame(summary(baseline_season_logit_model)$coef[summary(baseline_season_logit_model)$coef[,4]<=.05, 4])
# data.frame(summary(baseline_rolling_logit_model)$coef[summary(baseline_rolling_logit_model)$coef[,4]<=.05, 4])
```

\<code change, need to update!\> From the summary of the baseline
models, about 40 features (of the original 78) were statistically
significant. Of note, the coefficient of the factor of home_distance /
away_distance from our logit model initially confirmed the common
assumption that there is a home field advantage. These results may
change as we continue to develop our models.

#### Feature Selection Exploration

In order to explore feature selection, we looked at stepwise regression
using the `MASS` package as well as LASSO regression using `glmnet`. For
stepwise, we tested bidirectional elimination. For both feature
selection techniques, AIC was used as the model improvement criterion.
Both, stepwise and LASSO, produced similar results. Ultimately, LASSO
regression was selected due to overall performance due to how quickly it
returned results versus stepwise regression. Running stepwise regression
using large number of predictors was not as efficient as LASSO
regression due to how stepwise regression interactively adds and removes
predictors based on metrics until a subset of predictors is found. LASSO
regression provided us with a subset of predictors without sacrificing
performance training our models.

The training set was used to fit our lasso model which was created to
select our optimal set of predictors. Using the `cv.glmnet` function, we
performed 10-fold cross-validation on our lasso model.

To find the best lambda value for LASSO, we used
`cv.glmnet(family = "binomial", alpha = 1, nfolds = 10)`. We then used
the coefficients using the lambda value that was one standard error away
from that minimized the cross-validation error to select our significant
predictors from an original list of 78 potential predictors.

Altering the coefficient cutoff changed the selected predictors. We
changed the cutoffs multiple times until we found a good balance between
number of predictors remaining and logistic regression model
performance. Ultimately, we decided to limit our predictors to ones with
coefficient not equal to zero.

The results of our LASSO exploration upon the season averaged data led
to 30 features being selected (compared to 37 in the baseline). The
results of our LASSO exploration upon the rolling averaged data led to
44 features being selected (compared to 40 in the baseline). All of the
features selected in the season averaged LASSO were included in the
rolling averaged LASSO. Both included predictors such as: offensive
ratings, blocks, field goals attempted, assists, turnovers, and steals.
Of note is ratings and distance traveled are still chosen as significant
predictors for both LASSO feature selections.

#### Simple Models

After predictors were selected with LASSO regression, we trained a
logistic, probabilistic and gradient boosted decision tree models upon
both the rolling averaged and season averaged datasets. These so-called
"simple models" were trained with the features selected by LASSO
regression and no other processing of the data.

```{r simple_models, echo=FALSE, warning=FALSE, message=FALSE}
model_tbl %>% filter(Simplicity == "simple", Metric == "season", `Model Type` == "logit") %>% pull(test_confusion_matrix) %>% .[[1]] %>% draw_confusion_matrix()
```

Relative to the baseline logit model, the simple logit model had very
similar metrics. This was nice to see considering that the simple logit
model was trained on less than half of the features as the baseline
model. Therefore, increasing the generality of the model without
significantly sacrificing performance.

#### Enhanced Models

For our next set of models, our goals were to further protect against
overfitting and increasing generalization of our models by normalizing
the training data fed into the models. Data is also mapped to principal
components, which were automatically picked by the `tidymodels` package
to include at least 80% of the explained variance. We are also using
glmnet versions of the logit, meaning that the lambda is chosen
automatically to be the most optimal one. It still uses 10-fold cross
validation through `rsample::vfold_cv()`. The metric set that it is
optimizing on is ROC and AUC.

```{r enhanced_models, echo=FALSE, warning=FALSE, message=FALSE}

model_tbl %>% filter(Simplicity == "enhanced", Metric == "season", `Model Type` == "logit") %>% pull(test_confusion_matrix) %>% .[[1]] %>% draw_confusion_matrix()

```

The enhanced model made very small improvements to our accuracy,
specificity and precision (while minorly sacrificing sensitivity,
recall, F1 and Kappa) upon the validation dataset. This shows that the
enhanced processing was able to maintain the same levels of performances
as the baseline and simple logit models while retaining just over 80% of
the explained variance in the data.

#### Explain that we did more modeling besides Logit.

The baseline, simple and enhanced modeling procedures were how we
progressed a single type of model into its more generalized form. These
three procedures were also deployed upon other model types and datasets.
For instance, we: explored logit modeling using the rolling average
dataset (as opposed to the season average dataset), trained probit
models on both season average and rolling average datasets. We also
created extreme gradient boosted decision tree models. Finally, we
created an ensemble model from all our models. The results are in the
following table.

## Large Comparison Table

```{r comparison_table, echo=FALSE, warning=FALSE, message=FALSE}
model_tbl_final %>% filter(type %in% "validation") %>% select(-pred,-response,-confusion_matrix_) %>%
  unite("Model",c("Simplicity","Metric","Model Type"), sep = " ") %>% rename(Validation = type, Specificity = specificity_, Sensitivity = sensitivity_, Accuracy = accuracy_) %>% arrange(desc(Accuracy))%>% mutate(across(where(is.numeric), ~ scales::percent(.,accuracy = 0.01))) %>% mutate(Model = case_when(
    str_detect(Model,"espn") ~ "ESPN Pre-Game"
    , str_detect(Model,"combination") ~ "Ensemble Model"
    , TRUE ~ Model
  )) %>%
  ungroup() %>%
  select(-Validation) %>%
  kable() %>%
  kable_styling("striped")
```

#### Rolling vs Season

Throughout the analysis, we separated the rolling average data from the
season average data. Given metrics in the above table, we saw that there
were only minor differences between the performances of the same type of
model on the different training datasets. - The rolling average trained
models performed just a little worse, however, this shows that in order
to make a decently comparable prediction, you don't need to keep an
average of the statistics over the full season; instead, you can rely on
the data from the previous five games. - This would be useful for... -
helping somene jump into the middle of a season...

#### Logit vs Probit

We were also curious if there were any discrepancies between the logit
and probit models. From the results, you can see that the simple probit
model performed extremely closely with the simple logit model. This is
not an unexpected result as the two models can be used
interchangeably. - the most they deviated was when they were trained on
the \<...\> data and created as the "\<...\>." (simple, enhanced) -
Maybe will have to delete the previous statement because they were not
notably different.

#### Decision Trees

Decision trees represent our exploration into using a different type of
model (since logit and probit are extremely similar). Our boosted tree
models were created using the `parsnip::boost_tree()` function. The
hyperparameters that needed tuning were: depth of each tree, minimum
number of observations in each terminal node, the required reduction in
loss for a split to occur, the proportion of the training set to use for
each tree, the number of variables to consider for each split, and the
learning rate. To find the optimal hyperparameters, a latin hypercube
grid search was used; number of trees was held constant at 100.

#### Decision Trees vs Logit and Probit

-   Given the complexity of tuning the trees, our tree models were
    considered "enhanced" models. Therefore, there were no baseline or
    simple versions of these.
-   <Fact Check this> Just like the logit and probit models, the
    performances of the tree models did not different significantly when
    trained on either the season average data or the rolling average
    data.
-   <Fact Check this> The decision tree gave very similar results to the
    logit and probit models when trained in the same way.

#### Ensemble

The ensemble model was our way of aggregating all of the predictive
models into one. - It combined the \<...\> models by \<...\> - Ensemble
vs ESPN: Compared to the ESPN prediction accuracy, the ensemble model
performed <similarly> given the <season averaged training data>.

#### Modeling Challenges

During our Enhanced modeling exploratin, many of the models trained on
the rolling averages data would only predict a certain class (such as
predicting all wins, no loses, or vice versa). We believed this was
because the rolling averages features selected from LASSO were sparse
and not well correlated with game outcomes. For instance, some of the
predictors selected were "flagrant fouls," of which there were mostly
zeros in the data.

#### Overall Results

-   <fact check> Relative to each other, the models performed very
    similarly.
    -   <fact check> This seems to be the case due to the largest
        predictors were very prevalent in the models very early on.
-   <fact check> the features we added (rating and distance) provided
    significant information into the predictions of all the models we
    created.
-   In general, our modeling efforts were able to come close to ESPN's
    historical predictions
    -   and could be used as an alternative to ESPN's predictions.
-   Given ESPN's market strategy of being the number one source in
    sports experiences and knowledge, we'd say that our modeling efforts
    proved to be just as accurate.

#### Where to explore next:

-   This model might serve as a good basis for future exploration. There
    were many features that we were interested in incorporating, such
    as: time between games, individual player statistics and injuries
    data. Although distance traveled had a significant effect, we also
    hypothesized that we would be able to see an effect due to direction
    traveled (i.e. if a player traveled to another time zone and lost an
    hour of sleep).

-   We were also interested in fleshing out the results due to different
    feature selection methods. We explored bidirectional stepwise
    feature selection however opted to use the global optimization
    method of LASSO instead.

-   Through the progress report, we were planning on having a parallel
    study on predicting game attendance. However, upon generating the
    content for game outcomes, we realized that it would not be feasible
    to well-represent both analyses within this group project. We were
    interested in perhaps using projected attendance to build some sort
    of advertising CPM projections into March Madness.

-   Geographical Bias of Ratings

### Works Cited

-   [1] Bubel, Jennifer. "How Much Money Do Universities Get for Going
    to the NCAA March Madness Tournament?" *Diario AS*, 28 Feb. 2023,
    <https://en.as.com/ncaa/how-much-money-do-universities-get-for-going-to-the-ncaa-march-madness-tournament-n/.>

-   [2] Parker, Tim. "How Much Does the NCAA Make off March Madness?"
    Edited by Jefreda R Brown, *Investopedia*, Investopedia, 9 Mar.
    2023,
    <https://www.investopedia.com/articles/investing/031516/how-much-does-ncaa-make-march-madness.asp#:~:text=In%202022%2C%2045%20million%20Americans,see%20the%20heftiest%20cash%2Dout.>

-   [3] Pomeroy, Ken. "The Possession", *Kenpom*, 19 Mar. 2004,
    [https://kenpom.com/blog/the-possession/#:\\\~:text=The%20most%20common%20formula%20for,and%20FTA%20%3D%20free%20throw%20attempts](https://kenpom.com/blog/the-possession/#:~:text=The%20most%20common%20formula%20for,and%20FTA%20%3D%20free%20throw%20attempts){.uri}

-   [4] Korpar, Lora. "March Madness Betting Expected to Exceed \$3
    Billion, Set All-Time High", *Newsweek*, 14 Mar. 2022,
    <https://www.newsweek.com/march-madness-betting-expected-exceed-3-billion-set-all-time-high-1687917>

-   [5] Coleman, Madeline. "March Madness: How a fan used a machine to
    nail his bracket", *Sports Illustrated*, 31 Mar. 2021,
    <https://www.si.com/college/2021/03/31/march-madness-fan-trained-machine-predict-bracket-will-geoghegan>

-   [6] Consoli, John. "Advertisers Go Mad For March Madness", *TV News
    Check,* 16 Mar 2022,
    <https://tvnewscheck.com/business/article/advertisers-go-mad-for-march-madness/>
